---
title: "PIPAnalysis"
author: "Robert Feldstein"
date: "2025-02-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include = FALSE}
# Load Packages 
# Silence tidyverse warnings
suppressPackageStartupMessages(library(tidyverse))
library(openxlsx)
library(fixest) 
library(data.table)
library(zoo)
library(did)
library(lubridate)

```


```{r, include = FALSE}
# Load in the Dataset
df <- read.xlsx("../Data/FinalDataSet.xlsx")

# Create a time column

labels <- c("A1", "A2", "A3", "A4", "A5", "A6", "A7", "B1", "B2", "B3", "B4", 
            "B5", "B6", "C1", "C2", "C3", "C4", "C5", "C6", "D1","D2", 
            "D3", "D4", "D5", "D6")

# Create a mapping of labels to integers
label_mapping <- setNames(1:length(labels), labels)

# Convert strBlock column to integers
df$time<- label_mapping[df$strBlock]

```

# Are PIPs Effective?

One would expect that students who join PIPs should see their scores improved relative to had they not join a PIP. Studying the PIPs are particularly complicated by the fact that students join and exit PIPs at different times. It is even further complicated by students having different attributes, such as whether or not they have been employed in a hospital. We will explore this question using a variety of models.

## PIP Visualizations

Prior to analyzing the PIP data, we would like to get some visualizations on when students join the program and how long they stay involved.

```{r}
# We'd really like to make a column called "PIPStart" that is 1 if 
# the student started a PIP in that block and 0 otherwise
comparison <- df[,c("UniqueID", "strBlock", "medical", "Average", 
                    "PIPYN", "PIPActive", "Hosp", "time")]
comparison <- comparison %>% group_by(UniqueID) %>% 
  mutate(PIPStart = ifelse(PIPActive == 1 & lag(PIPActive) == 0, 1, 0))

# Now make a bar chart of the number of students who started a PIP in each block
grouped <- comparison %>% group_by(strBlock) %>% summarise(sum(PIPStart))

barplot(grouped$`sum(PIPStart)`, names.arg = grouped$strBlock, 
        col = "blue", main = "PIP Start by Block", xlab = "Block", 
        ylab = "Number of Students")


```

```{r}

grouped <- comparison %>% group_by(strBlock) %>% summarise(sum(PIPActive))

barplot(grouped$`sum(PIPActive)`, names.arg = grouped$strBlock, col = 
          "blue", main = "PIP Active by Block", xlab = "Block", 
        ylab = "Number of Students")


```

Mosaic Plots of Students in PIPs and whether or not they worked in a hospital.

```{r}
mosaicplot(table(comparison$PIPActive, comparison$Hosp), 
           main = "PIP Active vs. Medical", xlab = "PIP Active", 
           ylab = "Hospital Employment", color = c("grey", "brown"))
```

The mosaic plot shows that students who are enrolled in PIPs are typically not employed in the hospital. However, there is still a substantial number of students who are employed in the hospital and are enrolled in PIPs sometime during their time in the program. Due to this, we may need to control for hospital employment in our models.

\newpage 


## Difference in Difference Model: 

A DiD model is a useful tool for estimating the causal effect of a treatment or intervention. The model compares the average change in the outcome variable for the treatment group to the average change in the outcome variable for the control group. The difference between these two changes is the DiD estimate of the treatment effect. 

Its major problem in the context of PIPs is that it assumes that the treatment and control groups would have followed the same trend in the absence of the treatment. This is a strong assumption that may not actually be true. It is possible that students who join PIPs in general grow scores at a slower rate than that of students who do not join PIPs. However, in the absence of data regarding students who should have joined a PIP but did not, this is quite a good model to use. Another extreme problem is that a standard DiD model assumes that the treatment periods are the same for all students. This is not the case for PIPs. But for our first model, we will assume that all students started PIPs in A7.

```{r}

df$Post <- ifelse(df$time > 6, 1, 0)
didreg1 = lm(Average ~ PIPYN*Post, data = df)
summary(didreg1)
```

Interpretation:

(Intercept): The average score of students who did not join a PIP in the pre-treatment period (A1-A6).

PIPYN: The average difference in the score of students who joined a PIP relative to those who did not join a PIP in the pre-treatment period (A1-A6). This coefficient is negative, which means that students who joined a PIP had a lower average score relative to those who did not join a PIP in the pre-treatment period. This makes sense, as students who join PIPs are typically struggling with their studies.

Post: The average difference in the score of students in the post-treatment period (A7-D6) relative to the pre-treatment period (A1-A6). This coefficient is positive, which means that students, on average, saw an increase in their average score in the post-treatment period relative to the pre-treatment period. This makes sense, as students who are studying should be improving overtime.

(PIPYN:Post): This interaction variable represents the amount of EXTRA improvement in the average score of students who joined a PIP relative to those who did not join a PIP. The coefficient is positive, which means that students who joined a PIP saw an increase in their average score relative to those who did not join a PIP. However, this coefficient is not statistically significant meaning that the improvement is solely due to random chance.


```{r}
# What about controlling for hospital employment?

didreg2 = lm(Average ~ PIPYN*Post + Hosp, data = df)
summary(didreg2)
```

Interpretation:

The interpretation is the same as above, but now we have an additional Hosp coefficient. Hosp represents the average difference in the score of students who are employed in a hospital relative to those who are not employed in a hospital. It is positive, which means that students who are employed in a hospital have a higher average score relative to those who are not employed in a hospital. This coefficient is statistically significant, meaning that hospital employment has a significant effect on average student scores.


\newpage

## Staggered Difference In Difference Model:

In truth, it was a bit amateurish to assume that all PIPs started in A7. In reality, PIPs start at different times. 

Read the following article for information regarding staggered DiD's:
https://tilburgsciencehub.com/topics/analyze/causal-inference/did/staggered-did/


```{r}
df <- df %>%
  group_by(UniqueID) %>%
  mutate(PIPStart = ifelse(any(PIPActive == 1), min(time[PIPActive == 1]), 0)) %>%
  ungroup()
table(df$time, df$PIPStart)

```


```{r}

# Make a variable that indicates the period in which the student started a PIP
# This is the first period in which the student's PIPActive is a 1


filtered <- df %>%
  filter(!(PIPStart %in% c(5,12,15,17)))



staggered_nevertreated <- att_gt(yname = "Average", # outcome variable
                                  tname = "time", # time variable
                                  idname = "UniqueID", # id variable
                                  gname = "PIPStart", # first treatment period variable
                                  control_group = "nevertreated", # set the comparison group as either "never treated" or "not yet treated"
                                  data = filtered, # data
                                  xformla = NULL, # add covariates here but it is set to default in this case
                                  allow_unbalanced_panel = TRUE # indicate whether or not the function should balance with respect to time and id
)



```

```{r}
staggered_nevertreated_aggregate<- aggte(staggered_nevertreated, type = "dynamic", na.rm = TRUE)
summary(staggered_nevertreated_aggregate)

# Plot group-time ATTs
staggered_nevertreated_plot<- ggdid(staggered_nevertreated_aggregate)+ labs(x = "Time Relative to Starting a PIP", y = "Average Score Change")
print(staggered_nevertreated_plot)
```



\newpage

## Another Model:

Before we said we don't have data on students who should have joined a PIP but did not. However, we do have data on students who joined a PIP at different times! Effectively, we can use a model that compares these students to each other. 

```{r}

staggered_notyettreated <- att_gt(yname = "Average", # outcome variable
                                  tname = "time", # time variable
                                  idname = "UniqueID", # id variable
                                  gname = "PIPStart", # first treatment period variable
                                  control_group = "notyettreated", # set the comparison group as either "never treated" or "not yet treated"
                                  data = filtered, # data
                                  xformla = NULL, # add covariates here but it is set to default in this case
                                  allow_unbalanced_panel = TRUE # indicate whether or not the function should balance with respect to time and id
)

```

```{r}

staggered_notyettreated_aggregate<- aggte(staggered_notyettreated, type = "dynamic", na.rm = TRUE)
summary(staggered_notyettreated_aggregate)

# Plot group-time ATTs
staggered_notyettreated_plot<- ggdid(staggered_notyettreated_aggregate)+ labs(x = "Time Relative to Q&A Adoption (in 30-day bins)", y = "ATT")
print(staggered_notyettreated_plot)

```



\newpage

## Fixed Effects Model

```{r}
# What is the best way to analyze the PIP data with student 
# scores, given that students start PIPs at different times?

model <- feols(Average ~ PIPActive | UniqueID + time, data = df)
summary(model)

```


Well, we finally have a statistically significant result! What was the difference? Probably twofold. First, a fixed effects model does not have the same assumptions as a DiD model. It does not assume that the treatment and control groups would have followed the same trend in the absence of the treatment. Second, a fixed effects model controls for individual differences between students. This is important because students who join PIPs are likely to be different from those who do not join PIPs in many ways. By controlling for individual differences, we can get a more accurate estimate of the effect of PIPs on student scores.

Interpretation:

Being enrolled in a PIP is associated with a 0.12 point increase in student scores. This is a statistically significant result, meaning that it is unlikely to have occurred by random chance. This result suggests that PIPs have a positive effect on student scores. However, it is important to note that this is an observational study, so we cannot say for certain that PIPs themselves caused the increase in scores. It is possible that another change, related to being placed into a PIP (such as increased support from faculty or more time spent studying), caused the increase in scores.

```{r}
# Some kind of visualization of the Fixed Effects Model 


fe_values <- fixef(model)  # Extract fixed effects

# Convert to a data frame for visualization
fe_df <- data.frame(
  fe_id = unlist(fe_values),
  group = rep(names(fe_values), lengths(fe_values))
)

ggplot(fe_df, aes(x = fe_id)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  facet_wrap(~ group, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Fixed Effects", x = "Fixed Effect Estimate", y = "Count")


```


