---
title: "PIPAnalysis"
author: "Robert Feldstein"
date: "2025-02-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include = FALSE}
# Load Packages 
# Silence tidyverse warnings
suppressPackageStartupMessages(library(tidyverse))
library(openxlsx)
library(fixest) 
library(data.table)
library(zoo)
library(did)
library(lubridate)
library(survival)
library(stats)
library(sandwich)
library(lmtest)

```


```{r, include = FALSE}
# Load in the Dataset
df <- read.xlsx("../Data/FinalDataSet.xlsx")
#df <- read.xlsx("../Data/2025Data.xlsx")

# Create a time column

labels <- c("A1", "A2", "A3", "A4", "A5", "A6", "A7", "B1", "B2", "B3", "B4", 
            "B5", "B6", "C1", "C2", "C3", "C4", "C5", "C6", "D1","D2", 
            "D3", "D4", "D5", "D6")

# Create a mapping of labels to integers
label_mapping <- setNames(1:length(labels), labels)

# Convert strBlock column to integers
df$time<- label_mapping[df$strBlock]

```

# Are PIPs Effective?

One would expect that students who join PIPs should see their scores improved relative to had they not join a PIP. Studying the PIPs are particularly complicated by the fact that students join and exit PIPs at different times. It is even further complicated by students having different attributes, such as whether or not they have been employed in a hospital. We will explore this question using a variety of models.

## PIP Visualizations

Prior to analyzing the PIP data, we would like to get some visualizations on when students join the program and how long they stay involved.

```{r}
# We'd really like to make a column called "PIPStart" that is 1 if 
# the student started a PIP in that block and 0 otherwise

# df$PIPYN <- df$PIPOverall
# df$PIPActive <- df$PIPPrePost

comparison <- df[,c("UniqueID", "strBlock", "medical", "Average", 
                    "PIPYN", "PIPActive", "Hosp", "time")]




comparison <- comparison %>% group_by(UniqueID) %>% 
  mutate(PIPStart = ifelse(PIPActive == 1 & lag(PIPActive) == 0, 1, 0))

# Now make a bar chart of the number of students who started a PIP in each block
grouped <- comparison %>% group_by(strBlock) %>% summarise(sum(PIPStart))

barplot(grouped$`sum(PIPStart)`, names.arg = grouped$strBlock, 
        col = "blue", main = "PIP Start by Block", xlab = "Block", 
        ylab = "Number of Students")


```

```{r}

grouped <- comparison %>% group_by(strBlock) %>% summarise(sum(PIPActive))

barplot(grouped$`sum(PIPActive)`, names.arg = grouped$strBlock, col = 
          "blue", main = "PIP Active by Block", xlab = "Block", 
        ylab = "Number of Students")


```

Mosaic Plots of Students in PIPs and whether or not they worked in a hospital.

```{r}
mosaicplot(table(comparison$PIPActive, comparison$Hosp), 
           main = "PIP Active vs. Medical", xlab = "PIP Active", 
           ylab = "Hospital Employment", color = c("grey", "brown"))
```

The mosaic plot shows that students who are enrolled in PIPs are typically not employed in the hospital. However, there is still a substantial number of students who are employed in the hospital and are enrolled in PIPs sometime during their time in the program. Due to this, we may need to control for hospital employment in our models.

\newpage 

## Basic Linear and Logistic Models

```{r}
model.medical_linear <- lm(medical ~ PIPYN*time+Hosp, data = df)
summary(model.medical_linear)
```

This is just a simple linear regression; it does not try to measure the impact of PIPs amongst students before and after entering a PIP. It shows that students who are in PIPs on average have medical scores -0.34 points lower than that of other students, holding other variables constant.

```{r}
library(stats)
# Logistic Regression Model
model.medical_logistic <- glm(I(medical > 3) ~ PIPYN*time+Hosp , data = df, family = binomial(link = "logit"))
summary(model.medical_logistic)

```

This logistic regression model attempts to classify competent medical scores from non-competent ones. The coefficients are log-odds, with a negative log-odd indicating that the presence of being in a PIP decreases the chance of having a competent medical score.


```{r}
library(MASS)

pipdf <- df %>% filter(PIPYN==1)
# Fit an ordinal regression model
ord_model <- polr(as.factor(medical) ~ time + PIPActive, data = pipdf, method = "logistic")
summary(ord_model)

```

```{r}
library(lme4)

# Fit a mixed-effects model
model <- lmer(medical ~ time + PIPActive + (1 + time | UniqueID), data = pipdf)

summary(model)

```



\newpage


## Difference in Difference Model: 

A DiD model is a useful tool for estimating the causal effect of a treatment or intervention. The model compares the average change in the outcome variable for the treatment group to the average change in the outcome variable for the control group. The difference between these two changes is the DiD estimate of the treatment effect. 

Its major problem in the context of PIPs is that it assumes that the treatment and control groups would have followed the same trend in the absence of the treatment. This is a strong assumption that may not actually be true. It is possible that students who join PIPs in general grow scores at a slower rate than that of students who do not join PIPs. However, in the absence of data regarding students who should have joined a PIP but did not, this is quite a good model to use. Another extreme problem is that a standard DiD model assumes that the treatment periods are the same for all students. This is not the case for PIPs. But for our first model, we will assume that all students started PIPs in A7.

```{r}

df$Post <- ifelse(df$time > 6, 1, 0)
didreg1 = lm(Average ~ PIPYN*Post, data = df)
summary(didreg1)
```

Interpretation:

(Intercept): The average score of students who did not join a PIP in the pre-treatment period (A1-A6).

PIPYN: The average difference in the score of students who joined a PIP relative to those who did not join a PIP in the pre-treatment period (A1-A6). This coefficient is negative, which means that students who joined a PIP had a lower average score relative to those who did not join a PIP in the pre-treatment period. This makes sense, as students who join PIPs are typically struggling with their studies.

Post: The average difference in the score of students in the post-treatment period (A7-D6) relative to the pre-treatment period (A1-A6). This coefficient is positive, which means that students, on average, saw an increase in their average score in the post-treatment period relative to the pre-treatment period. This makes sense, as students who are studying should be improving overtime.

(PIPYN:Post): This interaction variable represents the amount of EXTRA improvement in the average score of students who joined a PIP relative to those who did not join a PIP. The coefficient is positive, which means that students who joined a PIP saw an increase in their average score relative to those who did not join a PIP. However, this coefficient is not statistically significant meaning that the improvement is solely due to random chance.


```{r}
# What about controlling for hospital employment?
df$Post <- ifelse(df$time > 6, 1, 0)
df <- df %>%
  group_by(UniqueID) %>%
  mutate(PIPStart = ifelse(any(PIPActive == 1), min(time[PIPActive == 1]), 0)) %>%
  ungroup()

# Try dropping the PIPStarts that are greater than 10
df_test <- df %>% filter(PIPStart <= 10)

didreg2 = lm(clinical_reasoning ~ PIPYN*Post + Hosp, data = df_test)
summary(didreg2)
```

Interpretation:

The interpretation is the same as above, but now we have an additional Hosp coefficient. Hosp represents the average difference in the score of students who are employed in a hospital relative to those who are not employed in a hospital. It is positive, which means that students who are employed in a hospital have a higher average score relative to those who are not employed in a hospital. This coefficient is statistically significant, meaning that hospital employment has a significant effect on average student scores.


\newpage

## Staggered Difference In Difference Model:

In truth, it was a bit amateurish to assume that all PIPs started in A7. In reality, PIPs start at different times. 

Read the following article for information regarding staggered DiD's:
https://tilburgsciencehub.com/topics/analyze/causal-inference/did/staggered-did/


```{r}
# df <- df %>%
#   group_by(UniqueID) %>%
#   mutate(PIPStart = ifelse(any(PIPActive == 1), min(time[PIPActive == 1]), 0)) %>%
#   ungroup()
# table(df$time, df$PIPStart)
pipdf <- df %>% filter(PIPYN==1)
pipdf <- pipdf %>%
  group_by(UniqueID) %>%
  mutate(PIPStart = ifelse(any(PIPActive == 1), min(time[PIPActive == 1]), 0)) %>%
  ungroup()
# table(pipdf$time, pipdf$PIPStart)

```


```{r}

# Make a variable that indicates the period in which the student started a PIP
# This is the first period in which the student's PIPActive is a 1


filtered <- df %>%
  filter(!(PIPStart %in% c(5,8, 10,11,12,13,14,15,17)))


staggered_nevertreated <- att_gt(yname = "medical", # outcome variable
                                  tname = "time", # time variable
                                  idname = "UniqueID", # id variable
                                  gname = "PIPStart", # first treatment period variable
                                  control_group = "nevertreated", # set the comparison group as either "never treated" or "not yet treated"
                                  data = filtered, # data
                                  xformla = NULL, # add covariates here but it is set to default in this case
                                  allow_unbalanced_panel = T,
                                 clustervars = c("UniqueID")# indicate whether or not the function should balance with respect to time and id
)



```

```{r}
staggered_nevertreated_aggregate<- aggte(staggered_nevertreated, type = "dynamic", na.rm = TRUE)
summary(staggered_nevertreated_aggregate)

# Plot group-time ATTs
staggered_nevertreated_plot<- ggdid(staggered_nevertreated_aggregate)+ labs(x = "Time Relative to Starting a PIP", y = "Average Score Change")
print(staggered_nevertreated_plot)
```



\newpage

## Another Model:

Before we said we don't have data on students who should have joined a PIP but did not. However, we do have data on students who joined a PIP at different times! Effectively, we can use a model that compares these students to each other. 

```{r}

staggered_notyettreated <- att_gt(yname = "Average", # outcome variable
                                  tname = "time", # time variable
                                  idname = "UniqueID", # id variable
                                  gname = "PIPStart", # first treatment period variable
                                  control_group = "notyettreated", # set the comparison group as either "never treated" or "not yet treated"
                                  data = filtered, # data
                                  xformla = NULL, # add covariates here but it is set to default in this case
                                  allow_unbalanced_panel = TRUE # indicate whether or not the function should balance with respect to time and id
)

```

```{r}

staggered_notyettreated_aggregate<- aggte(staggered_notyettreated, type = "dynamic", na.rm = TRUE)
summary(staggered_notyettreated_aggregate)

# Plot group-time ATTs
staggered_notyettreated_plot<- ggdid(staggered_notyettreated_aggregate)+ labs(x = "Time Relative to Q&A Adoption (in 30-day bins)", y = "ATT")
print(staggered_notyettreated_plot)

```



\newpage

## Fixed Effects Model

```{r}
# What is the best way to analyze the PIP data with student 
# scores, given that students start PIPs at different times?

model <- feols(Average ~ PIPActive | UniqueID + time, vcov = ~UniqueID, data = df)
summary(model)

# Do it again, but with a different dependent variable

model.medical <- feols(medical ~ PIPActive | UniqueID + time, vcov = ~UniqueID, data = df)
summary(model.medical)

model.clinical <- feols(clinical_reasoning ~ PIPActive | UniqueID + time, vcov = ~UniqueID, data = df)
summary(model.clinical)

model.professional <- feols(professionalism ~ PIPActive | UniqueID + time, vcov = ~UniqueID, data = df)
summary(model.professional)

model.collaboration <- feols(collaboration ~ PIPActive | UniqueID + time, vcov = ~UniqueID, data = df)
summary(model.collaboration)

```


Well, we finally have a statistically significant result! What was the difference? Probably twofold. First, a fixed effects model does not have the same assumptions as a DiD model. It does not assume that the treatment and control groups would have followed the same trend in the absence of the treatment. Second, a fixed effects model controls for individual differences between students. This is important because students who join PIPs are likely to be different from those who do not join PIPs in many ways. By controlling for individual differences, we can get a more accurate estimate of the effect of PIPs on student scores.

Interpretation:

Being enrolled in a PIP is associated with a 0.12 point increase in student scores. This is a statistically significant result, meaning that it is unlikely to have occurred by random chance. This result suggests that PIPs have a positive effect on student scores. However, it is important to note that this is an observational study, so we cannot say for certain that PIPs themselves caused the increase in scores. It is possible that another change, related to being placed into a PIP (such as increased support from faculty or more time spent studying), caused the increase in scores.


An issue with fixed effects regression:

It currently assumes that the difference between a 1 and a 2 is the same as the difference of a 3 and a 4. This is not necessarily true!


Workaround: Conditional Fixed Effects Ordered Logit

```{r}

df$medical_factor <- factor(df$medical, ordered = TRUE)
# df$clinical_factor <- factor(df$clinical_reasoning, ordered = TRUE)
# df$professional_factor <- factor(df$professionalism, ordered = TRUE)
# df$collaboration_factor <- factor(df$collaboration, ordered = TRUE)

# model.medical <- clogit(I(medical > 3) ~ PIPActive  + strata(UniqueID) + strata(time), data = df)
# summary(model.medical)

model.medical <- glm(I(medical > 3) ~ PIPActive *time , data = df, family = binomial(link = "logit"))

model.medical <- lm(I(medical>3) ~ PIPYN*time, data = df)

summary(model.medical)
# model.clinical <- clogit(I(clinical_reasoning > 3) ~ PIPActive * time + strata(UniqueID), data = df)
# summary(model.clinical)
# 
# model.professional <- clogit(I(professionalism > 3) ~ PIPActive * time + strata(UniqueID), data = df)
# summary(model.professional)
# 
# model.collaboration <- clogit(I(collaboration > 3) ~ PIPActive * time + strata(UniqueID), data = df)
# summary(model.collaboration)


```

Note: This model does not say anything about students not placed into a PIP. It only compares the difference in scores between students before they enter a PIP and after they enter a PIP.

When adding in time impacts, the PIPActive variable no longer is statistically significant at a 0.05 level. Not only that, but it is actually negative! This suggests that the PIPs may have a negative impact on student scores at time 0. However, looking at the interaction term (which also is not statistically significant) it would show that the impact of PIPS improves scores by about 3% greater every time unit. This would mean that PIPs become more effective over time. If we think about this logically, this makes sense. Students need time to adjust to the PIP and to see the benefits of it. In their first rotation there is no way they would see the same benefits as they would in their last rotation.


## Clustering Standard Errors


```{r}

# A correct way to do clustering

# Calculate clustered standard errors
cluster_se <- vcovCL(model.medical, cluster = ~ UniqueID)
# Summarize the model using clustered standard errors
summary_clustered <- coeftest(model.medical, vcov = cluster_se)
print(summary_clustered)


```
When utilizing clustering, again we do not see PIPs to be statistically significant. 

There are potentially other ways of representing competency; visualize boxplots between PIPs and Not in a PIP. 

```{r}
# Look at the distribution of scores for students in PIPs and not in PIPs

# Create a boxplot of scores for students in PIPs and not in PIPs

boxplot(df$medical ~ df$PIPActive, ylab = "Medical Scores", xlab = "PIP Status", main = "Medical Scores by PIP Status")
```





